{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zindi Project - Financial Inclusion in Africa\n",
    "\n",
    "Financial inclusion remains one of the main obstacles to economic and human development in Africa. For example, across Kenya, Rwanda, Tanzania, and Uganda only 9.1 million adults (or 14% of adults) have access to or use a commercial bank account.\n",
    "\n",
    "Traditionally, access to bank accounts has been regarded as an indicator of financial inclusion. Despite the proliferation of mobile money in Africa, and the growth of innovative fintech solutions, banks still play a pivotal role in facilitating access to financial services. Access to bank accounts enable households to save and make payments while also helping businesses build up their credit-worthiness and improve their access to loans, insurance, and related services. Therefore, access to bank accounts is an essential contributor to long-term economic growth.\n",
    "\n",
    "__The objective of this project is to create a machine learning model to predict which individuals are most likely to have or use a bank account.__ The models and solutions developed can provide an indication of the state of financial inclusion in Kenya, Rwanda, Tanzania and Uganda, while providing insights into some of the key factors driving individuals’ financial security.\n",
    "\n",
    "# Our Goal\n",
    "\n",
    "Our Goal is to predict values for our NaNs in our target column bank_account. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "| column | additional information |\n",
    "|--------|------------------------|\n",
    "| country | Country interviewee is in |\n",
    "| year | Year survey was done in  |\n",
    "| uniqueid | Unique identifier for each interviewee | \n",
    "| location_type | Type of location: Rural, Urban |\n",
    "| cellphone_access | If interviewee has access to a cellphone: Yes, No |\n",
    "| household_size | Number of people living in one house |\n",
    "| age_of_respondent | The age of the interviewee |\n",
    "| gender_of_respondent | Gender of interviewee: Male, Female | \n",
    "| relationship_with_head | The interviewee’s relationship with the head of the house:Head of Household, Spouse, Child, Parent, Other relative, Other non-relatives, Dont know |\n",
    "| marital_status | The martial status of the interviewee: Married/Living together, Divorced/Seperated, Widowed, Single/Never Married, Don’t know |\n",
    "| education_level | Highest level of education: No formal education, Primary education, Secondary education, Vocational/Specialised training, Tertiary education, Other/Dont know/RTA |\n",
    "| job_type | Type of job interviewee has: Farming and Fishing, Self employed, Formally employed Government, Formally employed Private, Informally employed, Remittance Dependent, Government Dependent, Other Income, No Income, Dont Know/Refuse to answer |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import chi2_contingency\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import imblearn\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy import stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data into a dataframe\n",
    "test = pd.read_csv('data/Test.csv')\n",
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Make a new Dataframe with all the data\n",
    "df = pd.concat([test, train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframe into a csv:\n",
    "\n",
    "# Define the path to the folder in your repository\n",
    "folder_path = 'data/'\n",
    "\n",
    "# Define the file name and extension\n",
    "file_name = 'data.csv'\n",
    "\n",
    "# Concatenate the folder path and file name\n",
    "file_path = f'{folder_path}/{file_name}'\n",
    "\n",
    "# Export the DataFrame to the specified folder\n",
    "df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Exploring the data\n",
    "\n",
    "In this part of the notebook we look and analyze our financial inclusion data we got from Zindi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the data\n",
    "print('Financial Inclusion dataset')\n",
    "print('==================')\n",
    "print('# observations: {}'.format(df.shape[0]))\n",
    "print('# features:     {}'.format(df.shape[1]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a concise summary of a DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column labels of the DataFrame.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for unique values in column bank_account\n",
    "df['bank_account'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for data imbalance\n",
    "df['bank_account'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have NaNs in the column bank account. Our goal is to fill this Nan values with values that our model (hopefully) predicts right.\n",
    "\n",
    "What we need to do is:\n",
    "* Create a data frame without NaNs. This will be the data we than will split into train and test data.\n",
    "* Create a data frame with all the NaN values. This will be the data we than will have our model predict with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame without the NaN in our target feature\n",
    "df_wo_target_nan = df.dropna()\n",
    "\n",
    "# set new index for our dataframe without the NaNs\n",
    "df_wo_target_nan = df_wo_target_nan.reset_index(drop=True)\n",
    "df_wo_target_nan.isnull().value_counts()\n",
    "df_wo_target_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new data frame with only the NaN in our target feature\n",
    "df_with_target_nan = df.where(df['bank_account'].isnull())\n",
    "df_with_target_nan.head()\n",
    "\n",
    "# Set a new index for our dataframe with the NaNs\n",
    "df_with_target_nan = df_with_target_nan.reset_index(drop=True)\n",
    "df_with_target_nan.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we now focus on our dataframe without the NaNs. \n",
    "Quick look at the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overview of all the unique values of the dataframe:\n",
    "for column in df_wo_target_nan.columns:\n",
    "    unique_values = df_wo_target_nan[column].unique()\n",
    "    print(f\"Column '{column}' has {len(unique_values)} unique value(s):\")\n",
    "    print(unique_values)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicate values\n",
    "print(f\"duplicate values in columns\")\n",
    "\n",
    "display(df_wo_target_nan.duplicated().value_counts())\n",
    "\n",
    "print('No duplicates found.')\n",
    "print(\"______\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of features \n",
    "\n",
    "features = df_wo_target_nan.columns.tolist()\n",
    "features.remove('bank_account')\n",
    "\n",
    "fig,ax = plt.subplots(4,3,figsize=(34,30))\n",
    "count = 0\n",
    "for item in features:\n",
    "    sns.histplot(df_wo_target_nan[item], kde=True, ax=ax[int(count/3)][count%3], color='#33658A').set(title=item, xlabel='')\n",
    "    count += 1\n",
    "ax.flat[-1].set_visible(False)\n",
    "fig.tight_layout(pad=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting correlation between numeric columns\n",
    "numeric_df = df_wo_target_nan.select_dtypes(include='number')\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(z=correlation_matrix.values, x=correlation_matrix.columns, y=correlation_matrix.index))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty matrix to store Cramér's V \n",
    "'''\n",
    "n_columns = len(df_wo_target_nan.columns)\n",
    "cramers_matrix = np.zeros((n_columns, n_columns))\n",
    "\n",
    "# Iterate over each pair of columns\n",
    "for i in range(n_columns):\n",
    "    for j in range(n_columns):\n",
    "        # Create a contingency table for the column pair\n",
    "        contingency_table = pd.crosstab(df_wo_target_nan.iloc[:, i], df.iloc[:, j])\n",
    "        \n",
    "        # Perform the chi-square test and calculate Cramér's V\n",
    "        chi2, _, _, _ = chi2_contingency(contingency_table)\n",
    "        n = len(df_wo_target_nan)\n",
    "        cramers_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
    "        \n",
    "        # Store the Cramér's V value in the matrix\n",
    "        cramers_matrix[i, j] = cramers_v\n",
    "\n",
    "# Create a DataFrame from the matrix with column names as indices and columns\n",
    "cramers_df = pd.DataFrame(cramers_matrix, index=df_wo_target_nan.columns, columns=df_wo_target_nan.columns)\n",
    "\n",
    "# Create a heatmap using Seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cramers_df, annot=True, fmt=\".2f\", cmap=\"crest\", square=True)\n",
    "plt.title(\"Cramér's V Heatmap\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics for specific columns with numerical features\n",
    "num_cols = [\"age_of_respondent\", \"household_size\", \"year\"]\n",
    "# check table again\n",
    "num_data = df_wo_target_nan[num_cols]\n",
    "print(\"______\"*30)\n",
    "display(num_data.head(10))\n",
    "print(\"______\"*30)\n",
    "display(num_data.describe())\n",
    "print(\"______\"*30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75% of the respondents have 49 years old. This could be explained by the fact that the average life expectancy in Africa is still very low compared to other continents. Concerning this column, there is also a big difference comparing with the maximum value of 100. There are some ouliers in these column, that must be analysed before modelling.\n",
    "<br>\n",
    "\n",
    "We can also predict outliers in the column ‘household_size’ due to the huge difference between the 75% of the respondents having 5 people living in their houses, and a maximum value of 21 people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = px.data.tips()\n",
    "fig = px.box(df_wo_target_nan, y='age_of_respondent')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = px.data.tips()\n",
    "fig = px.box(df_wo_target_nan, y='household_size')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out if there is a relation between the features (which contains objects) and our target feature, using the \"Cramers V\".\n",
    "\n",
    "* Small Effect:\n",
    "Cramér's V values close to 0 indicate a weak or negligible association between the categorical variables.\n",
    "\n",
    "* Medium Effect:\n",
    "Cramér's V values around 0.1 to 0.3 suggest a moderate association. This indicates that the variables have some degree of dependency, but the association may not be very strong.\n",
    "\n",
    "* Large Effect:\n",
    "Cramér's V values close to 0.3 or higher indicate a relatively strong association between the categorical variables. This suggests a notable dependency or relationship between the variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each column vs. the target column if there is a correlation by creating a function using the Cramér's V:\n",
    "\n",
    "# make a list with each column name \n",
    "column_names = df_wo_target_nan.columns.tolist()\n",
    "# delete bank_account from the list\n",
    "column_names.remove('bank_account')\n",
    "# create target value\n",
    "target_column = 'bank_account'\n",
    "\n",
    "def cramers_v(list, target_column):\n",
    "    \n",
    "    for name in list:\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(df_wo_target_nan[name], df_wo_target_nan[target_column])\n",
    "\n",
    "        # Perform chi-square test\n",
    "        chi2, p, *_ = chi2_contingency(contingency_table)\n",
    "\n",
    "        # Calculate Cramér's V\n",
    "        n = len(df_wo_target_nan)\n",
    "        cramers_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
    "\n",
    "        # only print output is cramers_v is bigger than 0.1\n",
    "        if cramers_v >= 0.1:\n",
    "\n",
    "            print('-----------------------------')\n",
    "            print(f'{name} vs. {target_column}')\n",
    "            # print(\"Chi-square:\", chi2)\n",
    "            # print(\"p-value:\", p)\n",
    "            print(\"Cramér's V:\", round(cramers_v, 3))\n",
    "        \n",
    "\n",
    "cramers_v(column_names, target_column)\n",
    "cramers_v(['job_type'], 'education_level')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is a medium correlation between job type and education level and a high correlation between job type/bank account and education level/bank account: let's make a plot to show it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each column vs. the target column if there is a correlation by creating a function using the Cramér's V:\n",
    "\n",
    "# make a list with each column name \n",
    "column_names = df_wo_target_nan.columns.tolist()\n",
    "# delete bank_account from the list\n",
    "column_names.remove('bank_account')\n",
    "column_names.remove('uniqueid')\n",
    "# create target value\n",
    "target_names = column_names\n",
    "\n",
    "def cramers_v(list_columns, list_targets):\n",
    "    \n",
    "    for name in list_columns:\n",
    "        # Create a contingency \n",
    "        for target in list_targets:\n",
    "            if name != target:\n",
    "                contingency_table = pd.crosstab(df_wo_target_nan[name], df_wo_target_nan[target])\n",
    "\n",
    "                # Perform chi-square test\n",
    "                chi2, p, *_ = chi2_contingency(contingency_table)\n",
    "\n",
    "                # Calculate Cramér's V\n",
    "                n = len(df_wo_target_nan)\n",
    "                cramers_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
    "\n",
    "                # only print output is cramers_v is bigger than 0.1\n",
    "                if cramers_v >= 0.3:\n",
    "\n",
    "                    print('-----------------------------')\n",
    "                    print(f'{name} vs. {target}')\n",
    "                    # print(\"Chi-square:\", chi2)\n",
    "                    # print(\"p-value:\", p)\n",
    "                    print(\"Cramér's V:\", round(cramers_v, 3))\n",
    "            \n",
    "\n",
    "cramers_v(column_names, target_names)\n",
    "# cramers_v(['job_type'], 'education_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot with bank account, hue=gender\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.countplot(x=df_wo_target_nan['bank_account'], hue=df_wo_target_nan['gender_of_respondent']);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "\n",
    "We now drop columns if they:\n",
    "\n",
    "* are an ID\n",
    "* have no to negligible correlation to the target feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming the columns for better readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column names\n",
    "df_wo_target_nan.rename(columns = {'country': 'country',\n",
    "        'year': 'year',\n",
    "        'uniqueid': 'id',\n",
    "        'location_type': 'location',\n",
    "        'cellphone_access': 'cellphone',\n",
    "        'household_size': 'household_size',\n",
    "        'age_of_respondent': 'age',\n",
    "        'gender_of_respondent': 'gender',\n",
    "        'relationship_with_head': 'relationship_with_head', \n",
    "        'marital_status': 'marital_status', \n",
    "        'education_level': 'education',\n",
    "        'job_type': 'job',\n",
    "        'bank_account': 'bank_account'},\n",
    "        inplace = True)\n",
    "\n",
    "df_wo_target_nan.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop id column\n",
    "df_wo_target_nan = df_wo_target_nan.drop('id', axis=1)\n",
    "df_wo_target_nan.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the target variable\n",
    "plt.title('Bank Account Count')\n",
    "sns.countplot(x=df_wo_target_nan.bank_account);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot which shows percentage of people with and without a bank account\n",
    "data = df_wo_target_nan['bank_account']\n",
    "\n",
    "plt.hist(data, weights=np.ones(len(data)) / len(data))\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan['bank_account'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two histogramm plots we can see that the data isn't well balanced. Out of 23.524 people in our dataset only 3.312 people (~18%) have a bank account. 20.212 don't.\n",
    "\n",
    "&rarr; We need to remove our majority class!\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot we can see that we have outliers in the household_size column and also in the age column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a pairplot to see how the variables differ depending on our target variable - 'bank_account'\n",
    "sns.pairplot(df_wo_target_nan, hue='bank_account', height=2);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We have many categorical values and a few discreet values:\n",
    "* create dummy variables (hot-one encoding for categorical values)\n",
    "* bin discreet values (discreet values)\n",
    "* create a csv file with the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('data/data_binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make values astyp int to properly use the RandomOverSampler\n",
    "df_final = df_final.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>household_size_1-2</th>\n",
       "      <th>household_size_2-5</th>\n",
       "      <th>household_size_5-10</th>\n",
       "      <th>household_size_10-25</th>\n",
       "      <th>age_0-20</th>\n",
       "      <th>age_20-40</th>\n",
       "      <th>age_40-70</th>\n",
       "      <th>age_70-100</th>\n",
       "      <th>country_Rwanda</th>\n",
       "      <th>country_Tanzania</th>\n",
       "      <th>...</th>\n",
       "      <th>job_Farming and Fishing</th>\n",
       "      <th>job_Formally employed Government</th>\n",
       "      <th>job_Formally employed Private</th>\n",
       "      <th>job_Government Dependent</th>\n",
       "      <th>job_Informally employed</th>\n",
       "      <th>job_No Income</th>\n",
       "      <th>job_Other Income</th>\n",
       "      <th>job_Remittance Dependent</th>\n",
       "      <th>job_Self employed</th>\n",
       "      <th>bank_account_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   household_size_1-2  household_size_2-5  household_size_5-10   \n",
       "0                   0                   1                    0  \\\n",
       "1                   0                   0                    1   \n",
       "2                   0                   0                    1   \n",
       "3                   0                   0                    1   \n",
       "4                   0                   0                    1   \n",
       "\n",
       "   household_size_10-25  age_0-20  age_20-40  age_40-70  age_70-100   \n",
       "0                     0         0          1          0           0  \\\n",
       "1                     0         0          0          0           1   \n",
       "2                     0         0          1          0           0   \n",
       "3                     0         0          1          0           0   \n",
       "4                     0         0          1          0           0   \n",
       "\n",
       "   country_Rwanda  country_Tanzania  ...  job_Farming and Fishing   \n",
       "0               0                 0  ...                        0  \\\n",
       "1               0                 0  ...                        0   \n",
       "2               0                 0  ...                        0   \n",
       "3               0                 0  ...                        0   \n",
       "4               0                 0  ...                        0   \n",
       "\n",
       "   job_Formally employed Government  job_Formally employed Private   \n",
       "0                                 0                              0  \\\n",
       "1                                 0                              0   \n",
       "2                                 0                              0   \n",
       "3                                 0                              1   \n",
       "4                                 0                              0   \n",
       "\n",
       "   job_Government Dependent  job_Informally employed  job_No Income   \n",
       "0                         0                        0              0  \\\n",
       "1                         1                        0              0   \n",
       "2                         0                        0              0   \n",
       "3                         0                        0              0   \n",
       "4                         0                        1              0   \n",
       "\n",
       "   job_Other Income  job_Remittance Dependent  job_Self employed   \n",
       "0                 0                         0                  1  \\\n",
       "1                 0                         0                  0   \n",
       "2                 0                         0                  1   \n",
       "3                 0                         0                  0   \n",
       "4                 0                         0                  0   \n",
       "\n",
       "   bank_account_Yes  \n",
       "0                 1  \n",
       "1                 0  \n",
       "2                 1  \n",
       "3                 0  \n",
       "4                 0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our target feature is unbalanced: we have an overfitting in our target category &rarr; no is the majority class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (16466, 39)\n",
      "y_train: (16466,)\n",
      "X_test: (7058, 39)\n",
      "y_test: (7058,)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN TEST SPLIT  \n",
    "# Defining X and y\n",
    "features = df_final.columns.tolist()\n",
    "features.remove('bank_account_Yes')\n",
    "# Target\n",
    "# y = heart.heart_attack\n",
    "# Predictors\n",
    "# X = heart.drop('heart_attack', axis=1) instead of making a list of features in line 2 and 3. So you don't have to drop the column in the dataframe\n",
    "\n",
    "X = df_final[features]\n",
    "y = df_final.bank_account_Yes\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "# Check the shape of the data sets\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use oversampling to balance out the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_train: (16466, 39)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y_train: (16466,)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'X_train_over: (28296, 39)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y_train_over: (28296,)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14148 14148\n"
     ]
    }
   ],
   "source": [
    "# Oversampling\n",
    "oversample = imblearn.over_sampling.RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "X_train_over, y_train_over = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "display('X_train: ' + str(X_train.shape))\n",
    "display('y_train: ' + str(y_train.shape))\n",
    "display('X_train_over: ' + str(X_train_over.shape))\n",
    "display('y_train_over: ' + str(y_train_over.shape))\n",
    "\n",
    "count_yes, count_no = 0, 0\n",
    "for i in y_train_over:\n",
    "    if i == 1:\n",
    "        count_yes += 1\n",
    "    if i == 0:\n",
    "        count_no += 1\n",
    "print (count_yes, count_no)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4964 1100]\n",
      " [ 362  632]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.82      0.87      6064\n",
      "           1       0.36      0.64      0.46       994\n",
      "\n",
      "    accuracy                           0.79      7058\n",
      "   macro avg       0.65      0.73      0.67      7058\n",
      "weighted avg       0.85      0.79      0.81      7058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here!\n",
    "y_pred_tree = dtree.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_tree))\n",
    "print(classification_report(y_test, y_pred_tree))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.77\n",
      "Accuracy on test set: 0.79\n",
      "--------------------------------------------------------------------------------\n",
      "Mean absolute error on train set: 0.23\n",
      "Mean absolute error on test set: 0.21\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression (base?) - line model:\n",
    "#df = pd.read_csv('data/data.csv')\n",
    "# create a new data frame without the NaN in our target feature\n",
    "#df_wo_target_nan = df.dropna(axis=0)\n",
    "# Defining X and y\n",
    "#features = df_wo_target_nan.columns.tolist()\n",
    "#features.remove('bank_account')\n",
    "#X = df_wo_target_nan[features]\n",
    "#y = df_wo_target_nan.bank_account\n",
    "# Splitting the dataset\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, shuffle=True)\n",
    "#Oversampling\n",
    "#oversample = imblearn.over_sampling.RandomOverSampler(sampling_strategy='minority')\n",
    "#X_train_over, y_train_over = oversample.fit_resample(X_train, y_train)\n",
    "# Encode categorical features\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "X_train_fitted_transformed = enc.fit_transform(X_train_over)\n",
    "X_test_fitted_transformed = enc.fit_transform(X_test)\n",
    "### Using Logistic regression using non-scaled data\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "log_reg.fit(X_train_fitted_transformed, y_train_over)\n",
    "y_pred_train = log_reg.predict(X_train_fitted_transformed)\n",
    "y_pred = log_reg.predict(X_test_fitted_transformed)\n",
    "# Print accuracy of our model\n",
    "print(\"Accuracy on train set:\", round(accuracy_score(y_train_over, y_pred_train), 2))\n",
    "print(\"Accuracy on test set:\", round(accuracy_score(y_test, y_pred), 2))\n",
    "print(\"--------\"*10)\n",
    "# Print MAE of our model\n",
    "y_pred_train_ = []\n",
    "y_pred_ = []\n",
    "for i in y_pred_train:\n",
    "    if i == 1: y_pred_train_.append(1)\n",
    "    if i == 0: y_pred_train_.append(0)\n",
    "for i in y_pred:\n",
    "    if i == 1: y_pred_.append(1)\n",
    "    if i == 0: y_pred_.append(0)\n",
    "print(\"Mean absolute error on train set:\", round(mean_absolute_error(y_train_over, y_pred_train_),2))\n",
    "print(\"Mean absolute error on test set:\", round(mean_absolute_error(y_test, y_pred_),2))\n",
    "print(\"--------\"*10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing the model and fitting it\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4992 1072]\n",
      " [ 355  639]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.82      0.87      6064\n",
      "           1       0.37      0.64      0.47       994\n",
      "\n",
      "    accuracy                           0.80      7058\n",
      "   macro avg       0.65      0.73      0.67      7058\n",
      "weighted avg       0.85      0.80      0.82      7058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "y_pred_forest = forest.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_forest))\n",
    "print(classification_report(y_test, y_pred_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahiwinderling/neuefische/Financial-Inclusion-Project/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/shahiwinderling/neuefische/Financial-Inclusion-Project/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/shahiwinderling/neuefische/Financial-Inclusion-Project/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END bootstrap=False, max_depth=10, max_features=auto, max_leaf_nodes=28, min_samples_split=10, n_estimators=87;, score=0.750 total time=   0.9s\n",
      "[CV 3/3] END bootstrap=False, max_depth=10, max_features=auto, max_leaf_nodes=28, min_samples_split=10, n_estimators=87;, score=0.760 total time=   0.8s\n",
      "[CV 2/3] END bootstrap=False, max_depth=10, max_features=auto, max_leaf_nodes=28, min_samples_split=10, n_estimators=87;, score=0.741 total time=   0.8s\n",
      "[CV 2/3] END bootstrap=False, max_depth=7, max_features=0.7, max_leaf_nodes=26, min_samples_split=10, n_estimators=153;, score=0.748 total time=   3.4s\n",
      "[CV 1/3] END bootstrap=False, max_depth=7, max_features=0.7, max_leaf_nodes=26, min_samples_split=10, n_estimators=153;, score=0.780 total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahiwinderling/neuefische/Financial-Inclusion-Project/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END bootstrap=True, max_depth=16, max_features=auto, max_leaf_nodes=43, min_samples_split=5, n_estimators=29;, score=0.757 total time=   0.3s\n",
      "[CV 1/3] END bootstrap=True, max_depth=12, max_features=0.7999999999999999, max_leaf_nodes=33, min_samples_split=2, n_estimators=188;, score=0.769 total time=   4.1s\n",
      "[CV 2/3] END bootstrap=True, max_depth=12, max_features=0.7999999999999999, max_leaf_nodes=33, min_samples_split=2, n_estimators=188;, score=0.752 total time=   4.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahiwinderling/neuefische/Financial-Inclusion-Project/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/shahiwinderling/neuefische/Financial-Inclusion-Project/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/shahiwinderling/neuefische/Financial-Inclusion-Project/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END bootstrap=False, max_depth=7, max_features=0.7, max_leaf_nodes=26, min_samples_split=10, n_estimators=153;, score=0.792 total time=   3.6s\n",
      "[CV 2/3] END bootstrap=True, max_depth=16, max_features=auto, max_leaf_nodes=43, min_samples_split=5, n_estimators=29;, score=0.760 total time=   0.3s\n",
      "[CV 3/3] END bootstrap=True, max_depth=12, max_features=0.7999999999999999, max_leaf_nodes=33, min_samples_split=2, n_estimators=188;, score=0.780 total time=   4.4s\n",
      "[CV 1/3] END bootstrap=True, max_depth=4, max_features=auto, max_leaf_nodes=17, min_samples_split=5, n_estimators=25;, score=0.711 total time=   0.2s\n",
      "[CV 3/3] END bootstrap=True, max_depth=16, max_features=auto, max_leaf_nodes=43, min_samples_split=5, n_estimators=29;, score=0.763 total time=   0.2s\n",
      "[CV 2/3] END bootstrap=True, max_depth=4, max_features=auto, max_leaf_nodes=17, min_samples_split=5, n_estimators=25;, score=0.700 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahiwinderling/neuefische/Financial-Inclusion-Project/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/shahiwinderling/neuefische/Financial-Inclusion-Project/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END bootstrap=True, max_depth=4, max_features=auto, max_leaf_nodes=17, min_samples_split=5, n_estimators=25;, score=0.740 total time=   0.2s\n",
      "[CV 1/3] END bootstrap=True, max_depth=16, max_features=0.7999999999999999, max_leaf_nodes=11, min_samples_split=5, n_estimators=25;, score=0.712 total time=   0.4s\n",
      "[CV 2/3] END bootstrap=True, max_depth=16, max_features=0.7999999999999999, max_leaf_nodes=11, min_samples_split=5, n_estimators=25;, score=0.658 total time=   0.4s\n",
      "[CV 3/3] END bootstrap=True, max_depth=16, max_features=0.7999999999999999, max_leaf_nodes=11, min_samples_split=5, n_estimators=25;, score=0.754 total time=   0.4s\n",
      "[CV 2/3] END bootstrap=True, max_depth=8, max_features=0.6, max_leaf_nodes=22, min_samples_split=5, n_estimators=83;, score=0.721 total time=   1.2s\n",
      "[CV 1/3] END bootstrap=True, max_depth=8, max_features=0.6, max_leaf_nodes=22, min_samples_split=5, n_estimators=83;, score=0.755 total time=   1.3s\n",
      "[CV 2/3] END bootstrap=False, max_depth=14, max_features=0.7999999999999999, max_leaf_nodes=40, min_samples_split=2, n_estimators=184;, score=0.752 total time=   5.6s\n",
      "[CV 1/3] END bootstrap=False, max_depth=14, max_features=0.7999999999999999, max_leaf_nodes=40, min_samples_split=2, n_estimators=184;, score=0.761 total time=   5.9s\n",
      "[CV 1/3] END bootstrap=True, max_depth=16, max_features=0.6, max_leaf_nodes=14, min_samples_split=5, n_estimators=188;, score=0.725 total time=   2.6s\n",
      "[CV 2/3] END bootstrap=True, max_depth=16, max_features=0.6, max_leaf_nodes=14, min_samples_split=5, n_estimators=188;, score=0.707 total time=   2.6s\n",
      "[CV 3/3] END bootstrap=True, max_depth=16, max_features=0.6, max_leaf_nodes=14, min_samples_split=5, n_estimators=188;, score=0.757 total time=   2.4s\n",
      "[CV 3/3] END bootstrap=True, max_depth=8, max_features=0.6, max_leaf_nodes=22, min_samples_split=5, n_estimators=83;, score=0.767 total time=   1.3s\n",
      "[CV 1/3] END bootstrap=False, max_depth=13, max_features=0.7, max_leaf_nodes=32, min_samples_split=10, n_estimators=79;, score=0.764 total time=   1.8s\n",
      "[CV 2/3] END bootstrap=False, max_depth=13, max_features=0.7, max_leaf_nodes=32, min_samples_split=10, n_estimators=79;, score=0.740 total time=   1.5s\n",
      "[CV 3/3] END bootstrap=False, max_depth=13, max_features=0.7, max_leaf_nodes=32, min_samples_split=10, n_estimators=79;, score=0.782 total time=   1.5s\n",
      "[CV 3/3] END bootstrap=False, max_depth=14, max_features=0.7999999999999999, max_leaf_nodes=40, min_samples_split=2, n_estimators=184;, score=0.784 total time=   5.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(random_state=50),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;bootstrap&#x27;: [True, False],\n",
       "                                        &#x27;max_depth&#x27;: [None, 3, 3, 3, 4, 4, 4, 5,\n",
       "                                                      5, 5, 6, 6, 6, 7, 7, 7, 8,\n",
       "                                                      8, 8, 9, 9, 9, 10, 10, 10,\n",
       "                                                      11, 11, 12, 12, 12, ...],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;, None,\n",
       "                                                         0.5, 0.6, 0.7,\n",
       "                                                         0.7999999999999999,\n",
       "                                                         0.8999999999999999],\n",
       "                                        &#x27;max_leaf_nodes&#x27;: [None, 10, 10, 10, 10,\n",
       "                                                           10, 10, 10, 1...\n",
       "                                                           10, 10, 10, 10, 11,\n",
       "                                                           11, 11, 11, 11, 11,\n",
       "                                                           11, 11, 11, 11, 11,\n",
       "                                                           11, 12, 12, 12, 12, ...],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: array([ 10,  13,  17,  21,  25,  29,  33,  37,  41,  44,  48,  52,  56,\n",
       "        60,  64,  68,  72,  75,  79,  83,  87,  91,  95,  99, 103, 106,\n",
       "       110, 114, 118, 122, 126, 130, 134, 137, 141, 145, 149, 153, 157,\n",
       "       161, 165, 168, 172, 176, 180, 184, 188, 192, 196, 200])},\n",
       "                   random_state=50, scoring=make_scorer(fbeta_score, beta=2),\n",
       "                   verbose=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(random_state=50),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;bootstrap&#x27;: [True, False],\n",
       "                                        &#x27;max_depth&#x27;: [None, 3, 3, 3, 4, 4, 4, 5,\n",
       "                                                      5, 5, 6, 6, 6, 7, 7, 7, 8,\n",
       "                                                      8, 8, 9, 9, 9, 10, 10, 10,\n",
       "                                                      11, 11, 12, 12, 12, ...],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;, None,\n",
       "                                                         0.5, 0.6, 0.7,\n",
       "                                                         0.7999999999999999,\n",
       "                                                         0.8999999999999999],\n",
       "                                        &#x27;max_leaf_nodes&#x27;: [None, 10, 10, 10, 10,\n",
       "                                                           10, 10, 10, 1...\n",
       "                                                           10, 10, 10, 10, 11,\n",
       "                                                           11, 11, 11, 11, 11,\n",
       "                                                           11, 11, 11, 11, 11,\n",
       "                                                           11, 12, 12, 12, 12, ...],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: array([ 10,  13,  17,  21,  25,  29,  33,  37,  41,  44,  48,  52,  56,\n",
       "        60,  64,  68,  72,  75,  79,  83,  87,  91,  95,  99, 103, 106,\n",
       "       110, 114, 118, 122, 126, 130, 134, 137, 141, 145, 149, 153, 157,\n",
       "       161, 165, 168, 172, 176, 180, 184, 188, 192, 196, 200])},\n",
       "                   random_state=50, scoring=make_scorer(fbeta_score, beta=2),\n",
       "                   verbose=5)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=50)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=50)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(random_state=50),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [None, 3, 3, 3, 4, 4, 4, 5,\n",
       "                                                      5, 5, 6, 6, 6, 7, 7, 7, 8,\n",
       "                                                      8, 8, 9, 9, 9, 10, 10, 10,\n",
       "                                                      11, 11, 12, 12, 12, ...],\n",
       "                                        'max_features': ['auto', 'sqrt', None,\n",
       "                                                         0.5, 0.6, 0.7,\n",
       "                                                         0.7999999999999999,\n",
       "                                                         0.8999999999999999],\n",
       "                                        'max_leaf_nodes': [None, 10, 10, 10, 10,\n",
       "                                                           10, 10, 10, 1...\n",
       "                                                           10, 10, 10, 10, 11,\n",
       "                                                           11, 11, 11, 11, 11,\n",
       "                                                           11, 11, 11, 11, 11,\n",
       "                                                           11, 12, 12, 12, 12, ...],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': array([ 10,  13,  17,  21,  25,  29,  33,  37,  41,  44,  48,  52,  56,\n",
       "        60,  64,  68,  72,  75,  79,  83,  87,  91,  95,  99, 103, 106,\n",
       "       110, 114, 118, 122, 126, 130, 134, 137, 141, 145, 149, 153, 157,\n",
       "       161, 165, 168, 172, 176, 180, 184, 188, 192, 196, 200])},\n",
       "                   random_state=50, scoring=make_scorer(fbeta_score, beta=2),\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RSEED = 50\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': np.linspace(10, 200).astype(int),\n",
    "    'max_depth': [None] + list(np.linspace(3, 20).astype(int)),\n",
    "    'max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),\n",
    "    'max_leaf_nodes': [None] + list(np.linspace(10, 50, 500).astype(int)),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Estimator for use in random search\n",
    "estimator = RandomForestClassifier(random_state = RSEED)\n",
    "# pimp the scoring for rs\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "scorer = make_scorer(fbeta_score, beta=2)\n",
    "# Create the random search model\n",
    "rs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n",
    "                        scoring = scorer, cv = 3, \n",
    "                        n_iter = 10, verbose = 5, random_state=RSEED)\n",
    "\n",
    "# Fit \n",
    "rs.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 153,\n",
       " 'min_samples_split': 10,\n",
       " 'max_leaf_nodes': 26,\n",
       " 'max_features': 0.7,\n",
       " 'max_depth': 7,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4307 1757]\n",
      " [ 214  780]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.71      0.81      6064\n",
      "           1       0.31      0.78      0.44       994\n",
      "\n",
      "    accuracy                           0.72      7058\n",
      "   macro avg       0.63      0.75      0.63      7058\n",
      "weighted avg       0.86      0.72      0.76      7058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_forest = rs.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_forest))\n",
    "print(classification_report(y_test, y_pred_forest))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| True Positives | True Negatives | Accuracy |\n",
    "|----------------|----------------|----------|\n",
    "|725 | 4923 | 0.80 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking Random Forest model with best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(bootstrap=False, max_depth=13, max_features=0.7,\n",
       "                       max_leaf_nodes=32, min_samples_split=10,\n",
       "                       n_estimators=79)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(bootstrap=False, max_depth=13, max_features=0.7,\n",
       "                       max_leaf_nodes=32, min_samples_split=10,\n",
       "                       n_estimators=79)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_depth=13, max_features=0.7,\n",
       "                       max_leaf_nodes=32, min_samples_split=10,\n",
       "                       n_estimators=79)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing the model with best params and fitting it\n",
    "forest = RandomForestClassifier(n_estimators=79, min_samples_split=10, max_leaf_nodes=32, max_features=0.7, max_depth=13, bootstrap=False)\n",
    "forest.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4652 1412]\n",
      " [ 235  759]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.77      0.85      6064\n",
      "           1       0.35      0.76      0.48       994\n",
      "\n",
      "    accuracy                           0.77      7058\n",
      "   macro avg       0.65      0.77      0.66      7058\n",
      "weighted avg       0.87      0.77      0.80      7058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "y_pred_forest = forest.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_forest))\n",
    "print(classification_report(y_test, y_pred_forest))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| True Positives | True Negatives | Accuracy | \n",
    "|----------------|----------------|----------|\n",
    "|734 | 4851 | 0.79 |\n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_neighbors': 9, 'weights': 'distance'}\n",
      "Best Score: 0.8269721267039525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.80      0.86      6064\n",
      "           1       0.33      0.62      0.43       994\n",
      "\n",
      "    accuracy                           0.77      7058\n",
      "   macro avg       0.63      0.71      0.65      7058\n",
      "weighted avg       0.84      0.77      0.80      7058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Separate the features (X) and the target variable (y)\n",
    "#X = df.drop('bank_account_Yes', axis=1) \n",
    "#y = df['bank_account_Yes']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform oversampling on the minority class\n",
    "oversampler = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Perform undersampling on the majority class\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9, 11, 15, 20], 'weights': ['uniform', 'distance']}\n",
    "\n",
    "# Create and train the KNN model with hyperparameter tuning\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best hyperparameters and corresponding score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Train the KNN model with the best hyperparameters\n",
    "best_knn = KNeighborsClassifier(**best_params)\n",
    "best_knn.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
