{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zindi Project - Financial Inclusion in Africa\n",
    "\n",
    "Financial inclusion remains one of the main obstacles to economic and human development in Africa. For example, across Kenya, Rwanda, Tanzania, and Uganda only 9.1 million adults (or 14% of adults) have access to or use a commercial bank account.\n",
    "\n",
    "Traditionally, access to bank accounts has been regarded as an indicator of financial inclusion. Despite the proliferation of mobile money in Africa, and the growth of innovative fintech solutions, banks still play a pivotal role in facilitating access to financial services. Access to bank accounts enable households to save and make payments while also helping businesses build up their credit-worthiness and improve their access to loans, insurance, and related services. Therefore, access to bank accounts is an essential contributor to long-term economic growth.\n",
    "\n",
    "__The objective of this project is to create a machine learning model to predict which individuals are most likely to have or use a bank account.__ The models and solutions developed can provide an indication of the state of financial inclusion in Kenya, Rwanda, Tanzania and Uganda, while providing insights into some of the key factors driving individuals’ financial security.\n",
    "\n",
    "# Our Goal\n",
    "\n",
    "Our Goal is to predict values for our NaNs in our target column bank_account. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "| column | additional information |\n",
    "|--------|------------------------|\n",
    "| country | Country interviewee is in |\n",
    "| year | Year survey was done in  |\n",
    "| uniqueid | Unique identifier for each interviewee | \n",
    "| location_type | Type of location: Rural, Urban |\n",
    "| cellphone_access | If interviewee has access to a cellphone: Yes, No |\n",
    "| household_size | Number of people living in one house |\n",
    "| age_of_respondent | The age of the interviewee |\n",
    "| gender_of_respondent | Gender of interviewee: Male, Female | \n",
    "| relationship_with_head | The interviewee’s relationship with the head of the house:Head of Household, Spouse, Child, Parent, Other relative, Other non-relatives, Dont know |\n",
    "| marital_status | The martial status of the interviewee: Married/Living together, Divorced/Seperated, Widowed, Single/Never Married, Don’t know |\n",
    "| education_level | Highest level of education: No formal education, Primary education, Secondary education, Vocational/Specialised training, Tertiary education, Other/Dont know/RTA |\n",
    "| job_type | Type of job interviewee has: Farming and Fishing, Self employed, Formally employed Government, Formally employed Private, Informally employed, Remittance Dependent, Government Dependent, Other Income, No Income, Dont Know/Refuse to answer |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import chi2_contingency\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from sklearn.model_selection import train_test_split \n",
    "import imblearn\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data into a dataframe\n",
    "test = pd.read_csv('data/Test.csv')\n",
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Make a new Dataframe with all the data\n",
    "df = pd.concat([test, train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframe into a csv:\n",
    "\n",
    "# Define the path to the folder in your repository\n",
    "folder_path = 'data/'\n",
    "\n",
    "# Define the file name and extension\n",
    "file_name = 'data.csv'\n",
    "\n",
    "# Concatenate the folder path and file name\n",
    "file_path = f'{folder_path}/{file_name}'\n",
    "\n",
    "# Export the DataFrame to the specified folder\n",
    "df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Exploring the data\n",
    "\n",
    "In this part of the notebook we look and analyze our financial inclusion data we got from Zindi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the data\n",
    "print('Financial Inclusion dataset')\n",
    "print('==================')\n",
    "print('# observations: {}'.format(df.shape[0]))\n",
    "print('# features:     {}'.format(df.shape[1]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a concise summary of a DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column labels of the DataFrame.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for unique values in column bank_account\n",
    "df['bank_account'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for data imbalance\n",
    "df['bank_account'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have NaNs in the column bank account. Our goal is to fill this Nan values with values that our model (hopefully) predicts right.\n",
    "\n",
    "What we need to do is:\n",
    "* Create a data frame without NaNs. This will be the data we than will split into train and test data.\n",
    "* Create a data frame with all the NaN values. This will be the data we than will have our model predict with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame without the NaN in our target feature\n",
    "df_wo_target_nan = df.dropna()\n",
    "\n",
    "# set new index for our dataframe without the NaNs\n",
    "df_wo_target_nan = df_wo_target_nan.reset_index(drop=True)\n",
    "df_wo_target_nan.isnull().value_counts()\n",
    "df_wo_target_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new data frame with only the NaN in our target feature\n",
    "df_with_target_nan = df.where(df['bank_account'].isnull())\n",
    "df_with_target_nan.head()\n",
    "\n",
    "# Set a new index for our dataframe with the NaNs\n",
    "df_with_target_nan = df_with_target_nan.reset_index(drop=True)\n",
    "df_with_target_nan.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we now focus on our dataframe without the NaNs. \n",
    "Quick look at the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overview of all the unique values of the dataframe:\n",
    "for column in df_wo_target_nan.columns:\n",
    "    unique_values = df_wo_target_nan[column].unique()\n",
    "    print(f\"Column '{column}' has {len(unique_values)} unique value(s):\")\n",
    "    print(unique_values)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicate values\n",
    "print(f\"duplicate values in columns\")\n",
    "\n",
    "display(df_wo_target_nan.duplicated().value_counts())\n",
    "\n",
    "print('No duplicates found.')\n",
    "print(\"______\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of features \n",
    "\n",
    "features = df_wo_target_nan.columns.tolist()\n",
    "features.remove('bank_account')\n",
    "\n",
    "fig,ax = plt.subplots(4,3,figsize=(34,30))\n",
    "count = 0\n",
    "for item in features:\n",
    "    sns.histplot(df_wo_target_nan[item], kde=True, ax=ax[int(count/3)][count%3], color='#33658A').set(title=item, xlabel='')\n",
    "    count += 1\n",
    "ax.flat[-1].set_visible(False)\n",
    "fig.tight_layout(pad=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting correlation between numeric columns\n",
    "numeric_df = df_wo_target_nan.select_dtypes(include='number')\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(z=correlation_matrix.values, x=correlation_matrix.columns, y=correlation_matrix.index))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty matrix to store Cramér's V \n",
    "'''\n",
    "n_columns = len(df_wo_target_nan.columns)\n",
    "cramers_matrix = np.zeros((n_columns, n_columns))\n",
    "\n",
    "# Iterate over each pair of columns\n",
    "for i in range(n_columns):\n",
    "    for j in range(n_columns):\n",
    "        # Create a contingency table for the column pair\n",
    "        contingency_table = pd.crosstab(df_wo_target_nan.iloc[:, i], df.iloc[:, j])\n",
    "        \n",
    "        # Perform the chi-square test and calculate Cramér's V\n",
    "        chi2, _, _, _ = chi2_contingency(contingency_table)\n",
    "        n = len(df_wo_target_nan)\n",
    "        cramers_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
    "        \n",
    "        # Store the Cramér's V value in the matrix\n",
    "        cramers_matrix[i, j] = cramers_v\n",
    "\n",
    "# Create a DataFrame from the matrix with column names as indices and columns\n",
    "cramers_df = pd.DataFrame(cramers_matrix, index=df_wo_target_nan.columns, columns=df_wo_target_nan.columns)\n",
    "\n",
    "# Create a heatmap using Seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cramers_df, annot=True, fmt=\".2f\", cmap=\"crest\", square=True)\n",
    "plt.title(\"Cramér's V Heatmap\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics for specific columns with numerical features\n",
    "num_cols = [\"age_of_respondent\", \"household_size\", \"year\"]\n",
    "# check table again\n",
    "num_data = df_wo_target_nan[num_cols]\n",
    "print(\"______\"*30)\n",
    "display(num_data.head(10))\n",
    "print(\"______\"*30)\n",
    "display(num_data.describe())\n",
    "print(\"______\"*30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75% of the respondents have 49 years old. This could be explained by the fact that the average life expectancy in Africa is still very low compared to other continents. Concerning this column, there is also a big difference comparing with the maximum value of 100. There are some ouliers in these column, that must be analysed before modelling.\n",
    "<br>\n",
    "\n",
    "We can also predict outliers in the column ‘household_size’ due to the huge difference between the 75% of the respondents having 5 people living in their houses, and a maximum value of 21 people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = px.data.tips()\n",
    "fig = px.box(df_wo_target_nan, y='age_of_respondent')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = px.data.tips()\n",
    "fig = px.box(df_wo_target_nan, y='household_size')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out if there is a relation between the features (which contains objects) and our target feature, using the \"Cramers V\".\n",
    "\n",
    "* Small Effect:\n",
    "Cramér's V values close to 0 indicate a weak or negligible association between the categorical variables.\n",
    "\n",
    "* Medium Effect:\n",
    "Cramér's V values around 0.1 to 0.3 suggest a moderate association. This indicates that the variables have some degree of dependency, but the association may not be very strong.\n",
    "\n",
    "* Large Effect:\n",
    "Cramér's V values close to 0.3 or higher indicate a relatively strong association between the categorical variables. This suggests a notable dependency or relationship between the variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each column vs. the target column if there is a correlation by creating a function using the Cramér's V:\n",
    "\n",
    "# make a list with each column name \n",
    "column_names = df_wo_target_nan.columns.tolist()\n",
    "# delete bank_account from the list\n",
    "column_names.remove('bank_account')\n",
    "# create target value\n",
    "target_column = 'bank_account'\n",
    "\n",
    "def cramers_v(list, target_column):\n",
    "    \n",
    "    for name in list:\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(df_wo_target_nan[name], df_wo_target_nan[target_column])\n",
    "\n",
    "        # Perform chi-square test\n",
    "        chi2, p, *_ = chi2_contingency(contingency_table)\n",
    "\n",
    "        # Calculate Cramér's V\n",
    "        n = len(df_wo_target_nan)\n",
    "        cramers_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
    "\n",
    "        # only print output is cramers_v is bigger than 0.1\n",
    "        if cramers_v >= 0.1:\n",
    "\n",
    "            print('-----------------------------')\n",
    "            print(f'{name} vs. {target_column}')\n",
    "            # print(\"Chi-square:\", chi2)\n",
    "            # print(\"p-value:\", p)\n",
    "            print(\"Cramér's V:\", round(cramers_v, 3))\n",
    "        \n",
    "\n",
    "cramers_v(column_names, target_column)\n",
    "cramers_v(['job_type'], 'education_level')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is a medium correlation between job type and education level and a high correlation between job type/bank account and education level/bank account: let's make a plot to show it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each column vs. the target column if there is a correlation by creating a function using the Cramér's V:\n",
    "\n",
    "# make a list with each column name \n",
    "column_names = df_wo_target_nan.columns.tolist()\n",
    "# delete bank_account from the list\n",
    "column_names.remove('bank_account')\n",
    "column_names.remove('uniqueid')\n",
    "# create target value\n",
    "target_names = column_names\n",
    "\n",
    "def cramers_v(list_columns, list_targets):\n",
    "    \n",
    "    for name in list_columns:\n",
    "        # Create a contingency \n",
    "        for target in list_targets:\n",
    "            if name != target:\n",
    "                contingency_table = pd.crosstab(df_wo_target_nan[name], df_wo_target_nan[target])\n",
    "\n",
    "                # Perform chi-square test\n",
    "                chi2, p, *_ = chi2_contingency(contingency_table)\n",
    "\n",
    "                # Calculate Cramér's V\n",
    "                n = len(df_wo_target_nan)\n",
    "                cramers_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
    "\n",
    "                # only print output is cramers_v is bigger than 0.1\n",
    "                if cramers_v >= 0.3:\n",
    "\n",
    "                    print('-----------------------------')\n",
    "                    print(f'{name} vs. {target}')\n",
    "                    # print(\"Chi-square:\", chi2)\n",
    "                    # print(\"p-value:\", p)\n",
    "                    print(\"Cramér's V:\", round(cramers_v, 3))\n",
    "            \n",
    "\n",
    "cramers_v(column_names, target_names)\n",
    "# cramers_v(['job_type'], 'education_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot with bank account, hue=gender\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.countplot(x=df_wo_target_nan['bank_account'], hue=df_wo_target_nan['gender_of_respondent']);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "\n",
    "We now drop columns if they:\n",
    "\n",
    "* are an ID\n",
    "* have no to negligible correlation to the target feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming the columns for better readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column names\n",
    "df_wo_target_nan.rename(columns = {'country': 'country',\n",
    "        'year': 'year',\n",
    "        'uniqueid': 'id',\n",
    "        'location_type': 'location',\n",
    "        'cellphone_access': 'cellphone',\n",
    "        'household_size': 'household_size',\n",
    "        'age_of_respondent': 'age',\n",
    "        'gender_of_respondent': 'gender',\n",
    "        'relationship_with_head': 'relationship_with_head', \n",
    "        'marital_status': 'marital_status', \n",
    "        'education_level': 'education',\n",
    "        'job_type': 'job',\n",
    "        'bank_account': 'bank_account'},\n",
    "        inplace = True)\n",
    "\n",
    "df_wo_target_nan.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop id column\n",
    "df_wo_target_nan = df_wo_target_nan.drop('id', axis=1)\n",
    "df_wo_target_nan.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the target variable\n",
    "plt.title('Bank Account Count')\n",
    "sns.countplot(x=df_wo_target_nan.bank_account);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot which shows percentage of people with and without a bank account\n",
    "data = df_wo_target_nan['bank_account']\n",
    "\n",
    "plt.hist(data, weights=np.ones(len(data)) / len(data))\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan['bank_account'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two histogramm plots we can see that the data isn't well balanced. Out of 23.524 people in our dataset only 3.312 people (~18%) have a bank account. 20.212 don't.\n",
    "\n",
    "&rarr; We need to remove our majority class!\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot we can see that we have outliers in the household_size column and also in the age column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a pairplot to see how the variables differ depending on our target variable - 'bank_account'\n",
    "sns.pairplot(df_wo_target_nan, hue='bank_account', height=2);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have many categorical values we have to use hot-one encoding. \n",
    "We are now creating our dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make dummy variables for our categorical columns\n",
    "\n",
    "cat_feats = ['country', 'year', 'location', 'cellphone', 'gender', 'relationship_with_head', 'marital_status', 'education', 'job', 'bank_account']\n",
    "df_dummy = pd.get_dummies(df_wo_target_nan, columns=cat_feats, drop_first=True)\n",
    "\n",
    "df_dummy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy.head(10).T\n",
    "# bin age and household size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our target feature is unbalanced: we have an overfitting in our target category &rarr; no is the majority class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN TEST SPLIT  \n",
    "# Defining X and y\n",
    "features = df_dummy.columns.tolist()\n",
    "features.remove('bank_account_Yes')\n",
    "# Target\n",
    "# y = heart.heart_attack\n",
    "# Predictors\n",
    "# X = heart.drop('heart_attack', axis=1) instead of making a list of features in line 2 and 3. So you don't have to drop the column in the dataframe\n",
    "\n",
    "X = df_dummy[features]\n",
    "y = df_dummy.bank_account_Yes\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Check the shape of the data sets\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling\n",
    "oversample = imblearn.over_sampling.RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "X_train_over, y_train_over = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "display('X_train: ' + str(X_train.shape))\n",
    "display('y_train: ' + str(y_train.shape))\n",
    "display('X_train_over: ' + str(X_train_over.shape))\n",
    "display('y_train_over: ' + str(y_train_over.shape))\n",
    "\n",
    "count_yes, count_no = 0, 0\n",
    "for i in y_train_over:\n",
    "    if i == 'Yes':\n",
    "        count_yes += 1\n",
    "    if i == 'No':\n",
    "        count_no += 1\n",
    "print (count_yes, count_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_over.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the model and fitting it\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test, train_label, test_label\n",
    "#X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "y_pred_forest = forest.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_forest))\n",
    "print(classification_report(y_test, y_pred_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSEED = 50\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': np.linspace(10, 200).astype(int),\n",
    "    'max_depth': [None] + list(np.linspace(3, 20).astype(int)),\n",
    "    'max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),\n",
    "    'max_leaf_nodes': [None] + list(np.linspace(10, 50, 500).astype(int)),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Estimator for use in random search\n",
    "estimator = RandomForestClassifier(random_state = RSEED)\n",
    "# pimp the scoring for rs\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "scorer = make_scorer(fbeta_score, beta=2)\n",
    "# Create the random search model\n",
    "rs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n",
    "                        scoring = scorer, cv = 3, \n",
    "                        n_iter = 10, verbose = 5, random_state=RSEED)\n",
    "\n",
    "# Fit \n",
    "rs.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_forest = rs.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_forest))\n",
    "print(classification_report(y_test, y_pred_forest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
