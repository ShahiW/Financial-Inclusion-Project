{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zindi Project - Financial Inclusion in Africa\n",
    "\n",
    "Financial inclusion remains one of the main obstacles to economic and human development in Africa. For example, across Kenya, Rwanda, Tanzania, and Uganda only 9.1 million adults (or 14% of adults) have access to or use a commercial bank account.\n",
    "\n",
    "Traditionally, access to bank accounts has been regarded as an indicator of financial inclusion. Despite the proliferation of mobile money in Africa, and the growth of innovative fintech solutions, banks still play a pivotal role in facilitating access to financial services. Access to bank accounts enable households to save and make payments while also helping businesses build up their credit-worthiness and improve their access to loans, insurance, and related services. Therefore, access to bank accounts is an essential contributor to long-term economic growth.\n",
    "\n",
    "__The objective of this project is to create a machine learning model to predict which individuals are most likely to have or use a bank account.__ The models and solutions developed can provide an indication of the state of financial inclusion in Kenya, Rwanda, Tanzania and Uganda, while providing insights into some of the key factors driving individuals’ financial security.\n",
    "\n",
    "# Our Goal\n",
    "\n",
    "Our Goal is to predict values for our NaNs in our target column bank_account. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "| column | additional information |\n",
    "|--------|------------------------|\n",
    "| country | Country interviewee is in |\n",
    "| year | Year survey was done in  |\n",
    "| uniqueid | Unique identifier for each interviewee | \n",
    "| location_type | Type of location: Rural, Urban |\n",
    "| cellphone_access | If interviewee has access to a cellphone: Yes, No |\n",
    "| household_size | Number of people living in one house |\n",
    "| age_of_respondent | The age of the interviewee |\n",
    "| gender_of_respondent | Gender of interviewee: Male, Female | \n",
    "| relationship_with_head | The interviewee’s relationship with the head of the house:Head of Household, Spouse, Child, Parent, Other relative, Other non-relatives, Dont know |\n",
    "| marital_status | The martial status of the interviewee: Married/Living together, Divorced/Seperated, Widowed, Single/Never Married, Don’t know |\n",
    "| education_level | Highest level of education: No formal education, Primary education, Secondary education, Vocational/Specialised training, Tertiary education, Other/Dont know/RTA |\n",
    "| job_type | Type of job interviewee has: Farming and Fishing, Self employed, Formally employed Government, Formally employed Private, Informally employed, Remittance Dependent, Government Dependent, Other Income, No Income, Dont Know/Refuse to answer |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from scipy.stats import chi2_contingency\n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data into a dataframe\n",
    "test = pd.read_csv('data/Test.csv')\n",
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Make a new Dataframe with all the data\n",
    "df = pd.concat([test, train])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting Dataframe into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframe into a csv:\n",
    "\n",
    "# Define the path to the folder in your repository\n",
    "folder_path = 'data/'\n",
    "\n",
    "# Define the file name and extension\n",
    "file_name = 'data.csv'\n",
    "\n",
    "# Concatenate the folder path and file name\n",
    "file_path = f'{folder_path}/{file_name}'\n",
    "\n",
    "# Export the DataFrame to the specified folder\n",
    "df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Exploring the data\n",
    "\n",
    "In this part of the notebook we look and analyze our financial inclusion data we got from Zindi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the data\n",
    "print('Financial Inclusion dataset')\n",
    "print('==================')\n",
    "print('# observations: {}'.format(df.shape[0]))\n",
    "print('# features:     {}'.format(df.shape[1]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a concise summary of a DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column labels of the DataFrame.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for unique values in column bank_account\n",
    "df['bank_account'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for data imbalance\n",
    "df['bank_account'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have NaNs in the column bank account. Our goal is to fill this Nan values with values that our model (hopefully) predicts right.\n",
    "\n",
    "What we need to do is:\n",
    "* Create a data frame without NaNs. This will be the data we than will split into train and test data.\n",
    "* Create a data frame with all the NaN values. This will be the data we than will have our model predict with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame without the NaN in our target feature\n",
    "df_wo_target_nan = df.dropna()\n",
    "\n",
    "# set new index for our dataframe without the NaNs\n",
    "df_wo_target_nan = df_wo_target_nan.reset_index(drop=True)\n",
    "df_wo_target_nan.isnull().value_counts()\n",
    "df_wo_target_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new data frame with only the NaN in our target feature\n",
    "df_with_target_nan = df.where(df['bank_account'].isnull())\n",
    "df_with_target_nan.head()\n",
    "\n",
    "# Set a new index for our dataframe with the NaNs\n",
    "df_with_target_nan = df_with_target_nan.reset_index(drop=True)\n",
    "df_with_target_nan.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we now focus on our dataframe without the NaNs. \n",
    "Quick look at the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overview of all the unique values of the dataframe:\n",
    "for column in df_wo_target_nan.columns:\n",
    "    unique_values = df_wo_target_nan[column].unique()\n",
    "    print(f\"Column '{column}' has {len(unique_values)} unique value(s):\")\n",
    "    print(unique_values)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overview of all the unique values of the dataframe:\n",
    "for column in df_wo_target_nan.columns:\n",
    "    unique_values = df_wo_target_nan[column].unique()\n",
    "    print(f\"Column '{column}' has {len(unique_values)} unique value(s):\")\n",
    "    print(unique_values)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicate values\n",
    "print(f\"duplicate values in columns\")\n",
    "\n",
    "display(df_wo_target_nan.duplicated().value_counts())\n",
    "\n",
    "print('No duplicates found.')\n",
    "print(\"______\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of features \n",
    "'''\n",
    "features = df_wo_target_nan.columns.tolist()\n",
    "features.remove('bank_account')\n",
    "\n",
    "fig,ax = plt.subplots(4,3,figsize=(34,30))\n",
    "count = 0\n",
    "for item in features:\n",
    "    sns.histplot(df_wo_target_nan[item], kde=True, ax=ax[int(count/3)][count%3], color='#33658A').set(title=item, xlabel='')\n",
    "    count += 1\n",
    "ax.flat[-1].set_visible(False)\n",
    "fig.tight_layout(pad=3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting correlation between numeric columns\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "numeric_df = df_wo_target_nan.select_dtypes(include='number')\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(z=correlation_matrix.values, x=correlation_matrix.columns, y=correlation_matrix.index))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out if there is a relation between the features (which contains objects) and our target feature, using the \"Cramers V\".\n",
    "\n",
    "* Small Effect:\n",
    "Cramér's V values close to 0 indicate a weak or negligible association between the categorical variables.\n",
    "\n",
    "* Medium Effect:\n",
    "Cramér's V values around 0.1 to 0.3 suggest a moderate association. This indicates that the variables have some degree of dependency, but the association may not be very strong.\n",
    "\n",
    "* Large Effect:\n",
    "Cramér's V values close to 0.3 or higher indicate a relatively strong association between the categorical variables. This suggests a notable dependency or relationship between the variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each column vs. the target column if there is a correlation by creating a function using the Cramér's V:\n",
    "\n",
    "# make a list with each column name \n",
    "column_names = df_wo_target_nan.columns.tolist()\n",
    "# delete bank_account from the list\n",
    "column_names.remove('bank_account')\n",
    "# create target value\n",
    "target_column = 'bank_account'\n",
    "\n",
    "def cramers_v(list):\n",
    "    \n",
    "    for name in column_names:\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(df_wo_target_nan[name], df_wo_target_nan[target_column])\n",
    "\n",
    "        # Perform chi-square test\n",
    "        chi2, p, *_ = chi2_contingency(contingency_table)\n",
    "\n",
    "        # Calculate Cramér's V\n",
    "        n = len(df_wo_target_nan)\n",
    "        cramers_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
    "\n",
    "        # only print output is cramers_v is bigger than 0.1\n",
    "        if cramers_v >= 0.1:\n",
    "\n",
    "            print('-----------------------------')\n",
    "            print(f'{name} vs. {target_column}')\n",
    "            # print(\"Chi-square:\", chi2)\n",
    "            # print(\"p-value:\", p)\n",
    "            print(\"Cramér's V:\", round(cramers_v, 3))\n",
    "        \n",
    "\n",
    "cramers_v(column_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "\n",
    "We now drop columns if they:\n",
    "\n",
    "* are an ID\n",
    "* have no to negligible correlation to the target feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming the columns for better readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column names\n",
    "df_wo_target_nan.rename(columns = {'country': 'country',\n",
    "        'year': 'year',\n",
    "        'uniqueid': 'id',\n",
    "        'location_type': 'location',\n",
    "        'cellphone_access': 'cellphone',\n",
    "        'household_size': 'household_size',\n",
    "        'age_of_respondent': 'age',\n",
    "        'gender_of_respondent': 'gender',\n",
    "        'relationship_with_head': 'relationship_with_head', \n",
    "        'marital_status': 'marital_status', \n",
    "        'education_level': 'education',\n",
    "        'job_type': 'job',\n",
    "        'bank_account': 'bank_account'},\n",
    "        inplace = True)\n",
    "\n",
    "df_wo_target_nan.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop id column\n",
    "df_wo_target_nan = df_wo_target_nan.drop('id', axis=1)\n",
    "df_wo_target_nan.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the target variable\n",
    "plt.title('Bank Account Count')\n",
    "sns.countplot(x=df_wo_target_nan.bank_account)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot which shows percentage of people with and without a bank account\n",
    "data = df_wo_target_nan['bank_account']\n",
    "\n",
    "plt.hist(data, weights=np.ones(len(data)) / len(data))\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two histogramm plots we can see that the data isn't well balanced. Out of 23.524 people in our dataset only 3.312 people (~18%) have a bank account. 20.212 don't.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Outliers without bank_account column\n",
    "column_names = df_wo_target_nan.columns.tolist()\n",
    "column_names.remove('bank_account')\n",
    "print(column_names)\n",
    "df_outliers = df_wo_target_nan[column_names].copy()\n",
    "\n",
    "\n",
    "\n",
    "df_outliers.plot(kind='box', subplots=True, layout=(4,3), figsize=(34,30))\n",
    "plt.show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot we can see that we have outliers in the household_size column and also in the age column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_target_nan['bank_account'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a pairplot to see how the variables differ depending on our target variable - 'bank_account'\n",
    "sns.pairplot(df_wo_target_nan, hue='bank_account', height=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# Create a new DataFrame that only includes the numerical variables\n",
    "df_numeric = df_wo_target_nan.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Compute correlations\n",
    "correlations = df_numeric.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(correlations)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(140, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "\n",
    "sns.heatmap(correlations, mask=mask, cmap=\"viridis\", vmax=1, annot=True,\n",
    "            linewidths=1, cbar_kws={\"shrink\": .7}, ax=ax);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "sns.set_context('notebook')\n",
    "sns.set(rc={'figure.figsize':(15, 6)})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns with categorical values are:\n",
    "\n",
    "* country\n",
    "* year\n",
    "* location\n",
    "* cellphone\n",
    "* gender\n",
    "* relationship_with_head\n",
    "* marital_status\n",
    "* education\n",
    "* job\n",
    "* bank_account"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have many categorical values we have to use hot-one encoding. \n",
    "We are now creating our dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make dummy variables for our categorical columns\n",
    "\n",
    "cat_feats = ['country', 'year', 'location', 'cellphone', 'gender', 'relationship_with_head', 'marital_status', 'education', 'job', 'bank_account']\n",
    "df_wo_target_nan = pd.get_dummies(df_wo_target_nan, columns=cat_feats, drop_first=True)\n",
    "\n",
    "df_wo_target_nan.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
